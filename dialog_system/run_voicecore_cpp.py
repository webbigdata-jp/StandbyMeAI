import asyncio
import httpx
import json
import re
import torch
from snac import SNAC
import scipy.io.wavfile as wavfile
import numpy as np
import pyaudio
import threading
import time
import subprocess
import sys
from transformers import AutoTokenizer
import queue

class VoiceCoreSystem:
    def __init__(self, 
                 gemma_server_url="http://localhost:8080",
                 voicecore_server_url="http://localhost:8081",
                 system_prompt=None):
        self.gemma_server_url = gemma_server_url
        self.voicecore_server_url = voicecore_server_url
        self.snac_model = None
        self.tokenizer = None
        self.system_prompt = system_prompt or "ã‚ãªãŸã¯è¦ªåˆ‡ã§å½¹ç«‹ã¤ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
        self.conversation_history = []
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°éŸ³å£°ç”¨ã®è¨­å®š
        self.audio_queue = queue.Queue()
        self.is_playing = False
        self.playback_thread = None
        self.sample_rate = 24000
        
        self.load_models()
    
    def load_models(self):
        """SNACãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿"""
        print("SNACãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
        self.snac_model = SNAC.from_pretrained("hubertsiuzdak/snac_24khz")
        self.snac_model.to("cpu")
        
        print("Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
        self.tokenizer = AutoTokenizer.from_pretrained("unsloth/gemma-3n-E2B-it")
    
    def format_chat_prompt(self, messages):
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’Gemma 3Nã®ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå½¢å¼ã«å¤‰æ›"""
        prompt = ""
        
        for i, message in enumerate(messages):
            is_last = (i == len(messages) - 1)
            role = message["role"]
            content = message["content"]
            
            if role in ["user", "system"]:
                prompt += f"<start_of_turn>user\n{content}<end_of_turn>\n"
                if is_last:
                    prompt += "<start_of_turn>model\n"
            
            elif role == "assistant":
                prompt += f"<start_of_turn>model\n{content}"
                if not is_last:
                    prompt += "<end_of_turn>\n"
        
        return prompt
    
    def set_system_prompt(self, system_prompt):
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­å®š"""
        self.system_prompt = system_prompt
        self.conversation_history = []
        print(f"ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­å®šã—ã¾ã—ãŸ: {system_prompt}")
    
    async def chat_with_gemma(self, user_input, use_history=True):
        """Gemma 3Nã¨ãƒãƒ£ãƒƒãƒˆ"""
        messages = []
        
        if self.system_prompt:
            messages.append({"role": "system", "content": self.system_prompt})
        
        if use_history and self.conversation_history:
            messages.extend(self.conversation_history)
        
        messages.append({"role": "user", "content": user_input})
        
        chat_prompt = self.format_chat_prompt(messages)
        
        print(f"\n=== ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ===\n{chat_prompt}\n=== ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ‚äº† ===\n")
        
        payload = {
            "prompt": chat_prompt,
            "temperature": 1.0,
            "top_k": 64,
            "top_p": 0.95,
            "min_p": 0.00,
            "repeat_penalty": 1.0,
            "n_predict": 512,
            "stop": ["<end_of_turn>", "<|end_of_text|>", "<start_of_turn>"]
        }
        
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    f"{self.gemma_server_url}/completion",
                    json=payload
                )
                
                print(f"Status Code: {response.status_code}")
                
                if response.status_code == 200:
                    result = response.json()
                    content = result.get('content', '').strip()
                    
                    if use_history:
                        self.conversation_history.append({"role": "user", "content": user_input})
                        self.conversation_history.append({"role": "assistant", "content": content})
                        
                        max_history_length = 10
                        if len(self.conversation_history) > max_history_length * 2:
                            self.conversation_history = self.conversation_history[-(max_history_length * 2):]
                    
                    return content
                else:
                    print(f"Gemmaã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {response.status_code}")
                    print(f"Response text: {response.text}")
                    return None
        except Exception as e:
            print(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def clear_conversation_history(self):
        """ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"""
        self.conversation_history = []
        print("ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
    
    def redistribute_codes(self, tokens):
        """ãƒˆãƒ¼ã‚¯ãƒ³ã‚’SNACã‚³ãƒ¼ãƒ‰å½¢å¼ã«å¤‰æ›"""
        code_list = [t - 128266 for t in tokens]
        layer_1, layer_2, layer_3 = [], [], []
        
        for i in range(len(code_list) // 7):
            layer_1.append(code_list[7*i])
            layer_2.append(code_list[7*i+1]-4096)
            layer_3.append(code_list[7*i+2]-(2*4096))
            layer_3.append(code_list[7*i+3]-(3*4096))
            layer_2.append(code_list[7*i+4]-(4*4096))
            layer_3.append(code_list[7*i+5]-(5*4096))
            layer_3.append(code_list[7*i+6]-(6*4096))
        
        return [
            torch.tensor(layer_1).unsqueeze(0),
            torch.tensor(layer_2).unsqueeze(0),
            torch.tensor(layer_3).unsqueeze(0)
        ]
    
    def snac_decode_to_audio(self, tokens):
        """SNACã§éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›"""
        if not tokens:
            return None
        
        code_length = (len(tokens) // 7) * 7
        if code_length == 0:
            return None
        
        tokens = tokens[:code_length]
        codes = self.redistribute_codes(tokens)
        
        with torch.inference_mode():
            audio_hat = self.snac_model.decode(codes)
            audio_np = audio_hat.detach().squeeze().cpu().numpy()
        
        return audio_np
    
    def start_playback_thread(self):
        """éŸ³å£°å†ç”Ÿã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹"""
        if self.playback_thread is None or not self.playback_thread.is_alive():
            self.is_playing = True
            while not self.audio_queue.empty():
                try:
                    self.audio_queue.get_nowait()
                except queue.Empty:
                    break
            self.playback_thread = threading.Thread(target=self._playback_worker, daemon=True)
            self.playback_thread.start()
            print("ğŸµ éŸ³å£°å†ç”Ÿã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹")
    
    def stop_playback_thread(self):
        """éŸ³å£°å†ç”Ÿã‚¹ãƒ¬ãƒƒãƒ‰ã‚’åœæ­¢"""
        self.is_playing = False
        self.audio_queue.put(None)
        if self.playback_thread and self.playback_thread.is_alive():
            self.playback_thread.join(timeout=1.0)
    
    def _playback_worker(self):
        """éŸ³å£°å†ç”Ÿãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œï¼‰"""
        try:
            p = pyaudio.PyAudio()
            stream = p.open(format=pyaudio.paFloat32,
                           channels=1,
                           rate=self.sample_rate,
                           output=True,
                           frames_per_buffer=2048)
            
            print("ğŸµ éŸ³å£°å†ç”Ÿãƒ¯ãƒ¼ã‚«ãƒ¼é–‹å§‹ - PyAudioåˆæœŸåŒ–å®Œäº†")
            
            chunk_count = 0
            
            while self.is_playing:
                try:
                    audio_data = self.audio_queue.get(timeout=1.0)
                    
                    if audio_data is None:
                        print("ğŸ›‘ éŸ³å£°å†ç”Ÿåœæ­¢ä¿¡å·ã‚’å—ä¿¡")
                        break
                    
                    chunk_count += 1
                    print(f"ğŸ”Š éŸ³å£°ãƒãƒ£ãƒ³ã‚¯#{chunk_count}å†ç”Ÿé–‹å§‹: {len(audio_data)}ã‚µãƒ³ãƒ—ãƒ«")
                    
                    if np.max(np.abs(audio_data)) > 0:
                        audio_data = audio_data / np.max(np.abs(audio_data)) * 0.8
                    
                    audio_bytes = audio_data.astype(np.float32).tobytes()
                    stream.write(audio_bytes)
                    print(f"âœ… éŸ³å£°ãƒãƒ£ãƒ³ã‚¯#{chunk_count}å†ç”Ÿå®Œäº†")
                    
                    self.audio_queue.task_done()
                    
                except queue.Empty:
                    print("â³ ã‚­ãƒ¥ãƒ¼å¾…æ©Ÿä¸­...")
                    continue
                except Exception as e:
                    print(f"âŒ éŸ³å£°å†ç”Ÿã‚¨ãƒ©ãƒ¼: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
            
            stream.stop_stream()
            stream.close()
            p.terminate()
            print(f"ğŸµ éŸ³å£°å†ç”Ÿãƒ¯ãƒ¼ã‚«ãƒ¼çµ‚äº† - ç·å†ç”Ÿãƒãƒ£ãƒ³ã‚¯æ•°: {chunk_count}")
            
        except Exception as e:
            print(f"âŒ éŸ³å£°å†ç”Ÿãƒ¯ãƒ¼ã‚«ãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
    
    async def generate_voice_tokens_streaming(self, text, callback=None):
        """
        VoiceCoreã§éŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ
        70ãƒˆãƒ¼ã‚¯ãƒ³æ¯ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å‘¼ã³å‡ºã—
        
        é‡è¦ãªä¿®æ­£: stream=true ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã€å®Ÿéš›ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å—ä¿¡
        """
        voice_prompt = f"<custom_token_3><|begin_of_text|>amitaro_female[neutral]: {text}<|eot_id|><custom_token_4><custom_token_5><custom_token_1>"
        
        payload = {
            "prompt": voice_prompt,
            "temperature": 0.8,
            "top_p": 0.95,
            "n_predict": 2048,
            "repeat_penalty": 1.1,
            "repeat_last_n": 70,
            "stream": True  # â˜…é‡è¦: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–
        }
        
        collected_tokens = []
        all_tokens = []
        
        try:
            async with httpx.AsyncClient(timeout=None) as client:
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å—ä¿¡
                async with client.stream(
                    'POST',
                    f"{self.voicecore_server_url}/completion",
                    json=payload,
                    headers={"Accept": "text/event-stream"}  # SSEã¾ãŸã¯NDJSONã«å¯¾å¿œ
                ) as response:
                    
                    print(f"ğŸ“¡ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ¥ç¶šç¢ºç«‹: {response.status_code}")
                    
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ãƒ‡ãƒ¼ã‚¿ã‚’å—ä¿¡
                    async for line in response.aiter_lines():
                        if line.strip():
                            # "data: " ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»ï¼ˆSSEã®å ´åˆï¼‰
                            if line.startswith("data: "):
                                line = line[6:]
                            
                            try:
                                data = json.loads(line)
                                
                                # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ½å‡º
                                if "content" in data:
                                    matches = re.findall(r'<custom_token_(\d+)>', data["content"])
                                    for match in matches:
                                        token_id = 128256 + int(match)
                                        collected_tokens.append(token_id)
                                        all_tokens.append(token_id)
                                        
                                        if len(collected_tokens) % 10 == 0:
                                            print(f"ğŸ¯ ãƒˆãƒ¼ã‚¯ãƒ³å—ä¿¡ä¸­: {len(collected_tokens)}/70")
                                        
                                        # 70ãƒˆãƒ¼ã‚¯ãƒ³æ¯ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ
                                        if len(collected_tokens) >= 70:
                                            processable_count = (len(collected_tokens) // 7) * 7
                                            if processable_count > 0:
                                                tokens_to_process = collected_tokens[:processable_count]
                                                remaining_tokens = collected_tokens[processable_count:]
                                                
                                                print(f"ğŸ¯ 70ãƒˆãƒ¼ã‚¯ãƒ³åˆ°é”! å‡¦ç†é–‹å§‹: {len(tokens_to_process)}ãƒˆãƒ¼ã‚¯ãƒ³")
                                                if callback:
                                                    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’éåŒæœŸã§å®Ÿè¡Œï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ã‚’é¿ã‘ã‚‹ï¼‰
                                                    await callback(tokens_to_process)
                                                
                                                collected_tokens = remaining_tokens
                                
                                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°çµ‚äº†ã®åˆ¤å®š
                                if data.get("stop", False):
                                    print("ğŸ“ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°çµ‚äº†ä¿¡å·ã‚’å—ä¿¡")
                                    break
                                    
                            except json.JSONDecodeError as e:
                                print(f"âš ï¸ JSONè§£æã‚¨ãƒ©ãƒ¼ï¼ˆç„¡è¦–ï¼‰: {e}")
                                continue
                            except Exception as e:
                                print(f"âŒ ãƒˆãƒ¼ã‚¯ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                                continue
                
                # æ®‹ã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚‚å‡¦ç†
                if collected_tokens:
                    processable_count = (len(collected_tokens) // 7) * 7
                    if processable_count > 0:
                        tokens_to_process = collected_tokens[:processable_count]
                        print(f"ğŸ“¦ æœ€çµ‚ãƒãƒƒãƒå‡¦ç†: {len(tokens_to_process)}ãƒˆãƒ¼ã‚¯ãƒ³")
                        if callback:
                            await callback(tokens_to_process)
            
            print(f"âœ… ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Œäº† - ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(all_tokens)}")
            return all_tokens
            
        except Exception as e:
            print(f"âŒ VoiceCoreã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    async def _streaming_audio_callback(self, tokens):
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°éŸ³å£°ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°"""
        try:
            print(f"ğŸ”„ éŸ³å£°å¤‰æ›é–‹å§‹: {len(tokens)}ãƒˆãƒ¼ã‚¯ãƒ³")
            audio_data = self.snac_decode_to_audio(tokens)
            if audio_data is not None:
                print(f"âœ… éŸ³å£°å¤‰æ›å®Œäº†: {len(audio_data)}ã‚µãƒ³ãƒ—ãƒ«")
                print(f"ğŸ“¨ ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºï¼ˆè¿½åŠ å‰ï¼‰: {self.audio_queue.qsize()}")
                
                self.audio_queue.put(audio_data)
                print(f"ğŸ“¨ éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚­ãƒ¥ãƒ¼ã«é€ä¿¡å®Œäº† - æ–°ã‚µã‚¤ã‚º: {self.audio_queue.qsize()}")
                
                # ã‚­ãƒ¥ãƒ¼ãŒå‡¦ç†ã•ã‚Œã‚‹ã®ã‚’å°‘ã—å¾…æ©Ÿ
                await asyncio.sleep(0.01)
            else:
                print(f"âŒ éŸ³å£°å¤‰æ›å¤±æ•—: ãƒˆãƒ¼ã‚¯ãƒ³æ•°={len(tokens)}")
        except Exception as e:
            print(f"âŒ éŸ³å£°å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
    
    def save_audio(self, audio_data, filename, sample_rate=24000):
        """éŸ³å£°ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if audio_data is not None:
            wavfile.write(filename, sample_rate, audio_data)
            print(f"éŸ³å£°ã‚’{filename}ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    async def process_conversation_streaming(self, user_input, save_file=None, use_history=True):
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°éŸ³å£°ä»˜ãä¼šè©±å‡¦ç†"""
        print(f"\nãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_input}")
        
        # Step 1: Gemma 3nã§å›ç­”ç”Ÿæˆ
        print("Gemma 3nã§å›ç­”ã‚’ç”Ÿæˆä¸­...")
        gemma_response = await self.chat_with_gemma(user_input, use_history=use_history)
        
        if not gemma_response:
            print("Gemmaã‹ã‚‰ã®å›ç­”ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        print(f"Gemmaå›ç­”: {gemma_response}")
        
        # Step 2: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°éŸ³å£°å†ç”Ÿé–‹å§‹
        print("ğŸµ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°éŸ³å£°ç”Ÿæˆãƒ»å†ç”Ÿã‚’é–‹å§‹...")
        
        # å†ç”Ÿã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
        self.start_playback_thread()
        
        try:
            # VoiceCoreã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°éŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ
            all_tokens = await self.generate_voice_tokens_streaming(
                gemma_response, 
                callback=self._streaming_audio_callback
            )
            
            print(f"ğŸ“Š ç·éŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(all_tokens)}")
            print(f"â±ï¸  éŸ³å£°ç”Ÿæˆå®Œäº† - å†ç”Ÿç¶™ç¶šä¸­...")
            
            # éŸ³å£°ã®å†ç”ŸãŒå®Œäº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ
            estimated_duration = (len(all_tokens) // 70) * 1.0 + 2.0
            print(f"â³ æ¨å®šå†ç”Ÿæ™‚é–“: {estimated_duration:.1f}ç§’")
            
            start_wait = time.time()
            while not self.audio_queue.empty():
                await asyncio.sleep(0.1)
                if time.time() - start_wait > estimated_duration:
                    print("âš ï¸  å†ç”Ÿã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                    break
            
            await asyncio.sleep(0.5)
            print("ğŸµ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°éŸ³å£°å†ç”Ÿå®Œäº†")
            
            # ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            if save_file and all_tokens:
                print("ğŸ’¾ å®Œå…¨ãªéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆãƒ»ä¿å­˜ä¸­...")
                complete_audio = self.snac_decode_to_audio(all_tokens)
                if complete_audio is not None:
                    self.save_audio(complete_audio, save_file)
            
            return all_tokens
            
        except Exception as e:
            print(f"âŒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return None
        finally:
            print("ğŸ›‘ éŸ³å£°å†ç”Ÿã‚¹ãƒ¬ãƒƒãƒ‰ã‚’åœæ­¢ä¸­...")
            self.stop_playback_thread()
    
    async def process_conversation(self, user_input, save_file=None, play_audio=True, use_history=True):
        """å¾“æ¥ã®ä¼šè©±å‡¦ç†ï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰"""
        print(f"\nãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_input}")
        
        print("Gemma 3nã§å›ç­”ã‚’ç”Ÿæˆä¸­...")
        gemma_response = await self.chat_with_gemma(user_input, use_history=use_history)
        
        if not gemma_response:
            print("Gemmaã‹ã‚‰ã®å›ç­”ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        print(f"Gemmaå›ç­”: {gemma_response}")
        
        print("VoiceCoreã§éŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆä¸­...")
        voice_tokens = await self.generate_voice_tokens_streaming(gemma_response)
        
        if not voice_tokens:
            print("éŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        print(f"éŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(voice_tokens)}")
        
        print("SNACã§éŸ³å£°åŒ–ä¸­...")
        audio_data = self.snac_decode_to_audio(voice_tokens)
        
        if audio_data is None:
            print("éŸ³å£°ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        if save_file:
            self.save_audio(audio_data, save_file)
        
        if play_audio:
            print("éŸ³å£°ã‚’å†ç”Ÿä¸­...")
            p = pyaudio.PyAudio()
            stream = p.open(format=pyaudio.paFloat32,
                           channels=1,
                           rate=self.sample_rate,
                           output=True)
            
            audio_bytes = audio_data.astype(np.float32).tobytes()
            stream.write(audio_bytes)
            
            stream.stop_stream()
            stream.close()
            p.terminate()
        
        return audio_data

# ä½¿ç”¨ä¾‹
async def main():
    custom_system_prompt = """ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨éŸ³å£°ã§ãŠã—ã‚ƒã¹ã‚Šã‚’ã—ã¦ã„ã‚‹ã®ã§è¿”ç­”ã¯å®Œçµã«ï¼‘æ–‡ã§è¿”ç­”ã—ã¾ã™ã€‚çµµæ–‡å­—ã‚„é¡”æ–‡å­—ã¯ä½¿ã£ã¦ã¯ã„ã‘ã¾ã›ã‚“"""
    
    voice_system = VoiceCoreSystem(system_prompt=custom_system_prompt)
    
    print("VoiceCoreçµ±åˆã‚·ã‚¹ãƒ†ãƒ ãŒèµ·å‹•ã—ã¾ã—ãŸï¼")
    print("ä½¿ç”¨æ–¹æ³•:")
    print("1. Gemmaã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•: ãƒãƒ¼ãƒˆ8080")
    print("2. VoiceCoreã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•: ãƒãƒ¼ãƒˆ8081")
    print("ã‚³ãƒãƒ³ãƒ‰:")
    print("  'quit' - çµ‚äº†")
    print("  'clear' - ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢")
    print("  'system' - ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¤‰æ›´")
    print("  'nohistory' - å±¥æ­´ãªã—ã§å˜ç™ºã®è³ªå•")
    print("  'streaming' - ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°éŸ³å£°ãƒ¢ãƒ¼ãƒ‰")
    print("  'normal' - é€šå¸¸éŸ³å£°ãƒ¢ãƒ¼ãƒ‰")
    
    streaming_mode = True
    
    while True:
        try:
            mode_indicator = "[ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°]" if streaming_mode else "[é€šå¸¸]"
            user_input = input(f"\n{mode_indicator}è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
            
            if user_input.lower() == 'quit':
                break
            
            elif user_input.lower() == 'clear':
                voice_system.clear_conversation_history()
                continue
            
            elif user_input.lower() == 'system':
                new_prompt = input("æ–°ã—ã„ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
                voice_system.set_system_prompt(new_prompt)
                continue
            
            elif user_input.lower() == 'streaming':
                streaming_mode = True
                print("ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ")
                continue
            
            elif user_input.lower() == 'normal':
                streaming_mode = False
                print("é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ")
                continue
            
            elif user_input.lower().startswith('nohistory '):
                actual_input = user_input[10:]
                timestamp = int(time.time())
                save_file = f"output_{timestamp}.wav"
                
                if streaming_mode:
                    await voice_system.process_conversation_streaming(
                        actual_input, 
                        save_file=save_file,
                        use_history=False
                    )
                else:
                    await voice_system.process_conversation(
                        actual_input, 
                        save_file=save_file, 
                        play_audio=True,
                        use_history=False
                    )
            
            else:
                timestamp = int(time.time())
                save_file = f"output_{timestamp}.wav"
                
                if streaming_mode:
                    await voice_system.process_conversation_streaming(
                        user_input, 
                        save_file=save_file,
                        use_history=True
                    )
                else:
                    await voice_system.process_conversation(
                        user_input, 
                        save_file=save_file, 
                        play_audio=True,
                        use_history=True
                    )
            
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            import traceback
            traceback.print_exc()
    
    print("ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã™")

if __name__ == "__main__":
    asyncio.run(main())